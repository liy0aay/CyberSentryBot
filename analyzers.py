"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏—à–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ VirusTotal –∏ NLP.

–ö–ª–∞—Å—Å—ã:
    - VirusTotalClient: –ü—Ä–æ–≤–µ—Ä–∫–∞ URL —á–µ—Ä–µ–∑ VirusTotal API
    - BaseAnalyzer: –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π
    - PhishingAnalyzer: –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ñ–∏—à–∏–Ω–≥

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    - –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ —Ñ–∏—à–∏–Ω–≥/–±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    - –ü—Ä–æ–≤–µ—Ä–∫–∞ URL-–∞–¥—Ä–µ—Å–æ–≤ —á–µ—Ä–µ–∑ VirusTotal API —Å –≤—ã–≤–æ–¥–æ–º —É—Ä–æ–≤–Ω—è —É–≥—Ä–æ–∑—ã


–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - Python 3.8+
    - requests
    - transformers
    - torch
    - deep-translator
"""
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import re
from deep_translator import GoogleTranslator
from typing import List
import base64
from urllib.parse import urlparse
import requests

class BaseAnalyzer:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π."""
    pass

class VirusTotalClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å VirusTotal API.

    –ê—Ç—Ä–∏–±—É—Ç—ã:
        api_key (str): API –∫–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ VirusTotal.
        base_url (str): –ë–∞–∑–æ–≤—ã–π URL —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ API.
    """
    def __init__(self, api_key: str):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ VirusTotal.

        Args:
            api_key (str): API –∫–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ VirusTotal.
        """
        self.api_key = api_key
        self.base_url = "https://www.virustotal.com/api/v3/urls"
        

    def check_url(self, url: str) -> str:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç URL —á–µ—Ä–µ–∑ API VirusTotal –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å.

        Args:
            url (str): –°—Å—ã–ª–∫–∞ –∏–ª–∏ –¥–æ–º–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.

        Returns:
            str: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞, –≤–∫–ª—é—á–∞—è —É—Ä–æ–≤–µ–Ω—å —É–≥—Ä–æ–∑—ã –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ.
        """
        try:
            url_id = base64.urlsafe_b64encode(url.encode()).decode().strip("=")
            headers = {"x-apikey": self.api_key}
            response = requests.get(f"{self.base_url}/{url_id}", headers=headers)
            if response.status_code == 200:
                data = response.json()
                malicious = data.get("data", {}).get("attributes", {}).get("last_analysis_stats", {}).get("malicious", 0)
                if malicious > 0:
                    return f"‚ö†Ô∏è –í—Ä–µ–¥–æ–Ω–æ—Å–Ω–∞—è —Å—Å—ã–ª–∫–∞ ({malicious} –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–æ–≤ –æ—Ç–º–µ—Ç–∏–ª–∏ –∫–∞–∫ –æ–ø–∞—Å–Ω—É—é)"
                else:
                    return "‚úÖ –°—Å—ã–ª–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–∞"
            elif response.status_code == 404:
                return "‚ÑπÔ∏è –°—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ VirusTotal"
            else:
                return f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–∫–∏: {response.status_code}"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Å—ã–ª–∫–∏: {e}"

class PhishingAnalyzer(BaseAnalyzer):
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ñ–∏—à–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π NLP –º–æ–¥–µ–ª—å –∏ VirusTotal API.

    –ê—Ç—Ä–∏–±—É—Ç—ã:
        vt_client (VirusTotalClient): –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å VirusTotal.
        nlp_model: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è NLP –º–æ–¥–µ–ª—å (transformers).
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏.
    """

    def __init__(self, vt_client: VirusTotalClient, nlp_model=None, tokenizer=None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ñ–∏—à–∏–Ω–≥–∞.

        Args:
            vt_client (VirusTotalClient): –≠–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∏–µ–Ω—Ç–∞ VirusTotal.
            nlp_model: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è NLP –º–æ–¥–µ–ª—å.
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è NLP –º–æ–¥–µ–ª–∏.
        """
        self.vt_client = vt_client
        self.nlp_model = nlp_model
        self.tokenizer = tokenizer

    def extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω–Ω–æ–µ –∏–º—è –∏–∑ URL.

        Args:
            url (str): URL —Å—Ç—Ä–æ–∫–∞.

        Returns:
            str: –î–æ–º–µ–Ω –∏–∑ URL.
        """
        if not re.match(r'^[a-zA-Z][a-zA-Z0-9+\-.]*://', url):
            url = 'http://' + url
        parsed = urlparse(url)
        return parsed.netloc

    def _extract_urls(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ URL-–∞–¥—Ä–µ—Å–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

        Args:
            text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç.

        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ URL-–∞–¥—Ä–µ—Å–æ–≤, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤ —Ç–µ–∫—Å—Ç–µ.
        """
        url_pattern = (
            r'(?i)\b('
            r'(?:https?://|ftp://)?'  # –ø—Ä–æ—Ç–æ–∫–æ–ª 
            r'(?:www\.)?'  # www 
            r'[a-z0-9\-._~%]+'
            r'(?:\.[a-z]{2,})+'  # –¥–æ–º–µ–Ω
            r'(?:/[^\s]*)?'  # –ø—É—Ç—å 
            r')'
        )
        return [u for u in re.findall(url_pattern, text) if '.' in u]

    def analyze_message(self, text: str) -> List[str]:
        """–ü—Ä–æ–≤–æ–¥–∏—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ —Ñ–∏—à–∏–Ω–≥.

        Args:
            text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è.

        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ (—Ç–µ–∫—Å—Ç –∏ —Å—Å—ã–ª–∫–∏).
        """
        results = []

        text_result = self.analyze_text(text)
        if 'error' in text_result:
            return [f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞: {text_result['error']}"]

        results.append(f"üìù –¢–µ–∫—Å—Ç: {text_result['verdict']} ({text_result['confidence']:.1%})")

        urls = self._extract_urls(text)
        
        for url in urls:
            domain = self.extract_domain(url)
            url_result = self.vt_client.check_url(domain)
            results.append(f"üîó –°—Å—ã–ª–∫–∞: {url_result}")
        return results

    def analyze_text(self, text: str) -> dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∫–∞–∫ —Ñ–∏—à–∏–Ω–≥ –∏–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é.

        Args:
            text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç.

        Returns:
            dict: –°–ª–æ–≤–∞—Ä—å —Å –≤–µ—Ä–¥–∏–∫—Ç–æ–º (`phishing` –∏–ª–∏ `safe`), —É—Ä–æ–≤–Ω–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                  –∏ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏ (–æ—Ü–µ–Ω–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º).
        """
        if re.search(r'[–∞-—è–ê-–Ø]', text):
            try:
                text = GoogleTranslator(source='auto', target='en').translate(text)
            except Exception as e:
                return {'error': f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}"}

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )

        with torch.no_grad():
            outputs = self.nlp_model(**inputs)
            temperature = 2.0
            scaled_logits = outputs.logits / temperature
            probs = torch.nn.functional.softmax(scaled_logits, dim=-1).squeeze()

        phishing_score = probs[1] + probs[3]
        legitimate_score = probs[0] + probs[2] 

        verdict = 'phishing' if phishing_score > 0.7 else 'safe'
        confidence = max(phishing_score, legitimate_score)
        print (f'{verdict}: —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {confidence}%')

        return {
            'verdict': verdict,
            'confidence': confidence,
            'details': {
                'legitimate_email': f"{probs[0]:.1%}",
                'phishing_url': f"{probs[1]:.1%}",
                'legitimate_url': f"{probs[2]:.1%}",
                'phishing_url_alt': f"{probs[3]:.1%}"
            }
        }
    
    

    

